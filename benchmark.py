#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬
æ¯”è¾ƒ html.parser å’Œ lxml çš„è§£ææ€§èƒ½
"""

import time
import requests
from bs4 import BeautifulSoup


def fetch_html():
    """è·å–æœ‰é“è¯å…¸é¡µé¢ HTML"""
    url = "https://dict.youdao.com/search?q=hello"
    headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    }
    response = requests.get(url, headers=headers, timeout=10)
    return response.text


def benchmark_parser(html, parser_name, parser_type, iterations=100):
    """æµ‹è¯•è§£æå™¨æ€§èƒ½"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•è§£æå™¨: {parser_name}")
    print(f"{'='*60}")

    # é¢„çƒ­ï¼ˆç¬¬ä¸€æ¬¡è§£æå¯èƒ½è¾ƒæ…¢ï¼‰
    soup = BeautifulSoup(html, parser_type)
    soup.find('div', id='results-contents')

    # æ­£å¼æµ‹è¯•
    start_time = time.time()
    for i in range(iterations):
        soup = BeautifulSoup(html, parser_type)
        results_contents = soup.find('div', id='results-contents')
        if results_contents:
            trans_container = results_contents.find('div', class_='trans-container')
            if trans_container:
                translation_items = trans_container.find_all('li')
                translations = [item.get_text(strip=True) for item in translation_items if item.get_text(strip=True)]

    end_time = time.time()
    total_time = end_time - start_time
    avg_time = total_time / iterations

    print(f"è¿­ä»£æ¬¡æ•°: {iterations}")
    print(f"æ€»è€—æ—¶: {total_time:.3f} ç§’")
    print(f"å¹³å‡æ¯æ¬¡: {avg_time:.4f} ç§’")
    print(f"æ¯ç§’è§£æ: {1/avg_time:.1f} æ¬¡")

    return total_time, avg_time


def compare_parsers():
    """å¯¹æ¯”ä¸¤ç§è§£æå™¨"""
    print("="*60)
    print("æœ‰é“è¯å…¸çˆ¬è™« - è§£æå™¨æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*60)

    # è·å– HTML
    print("\næ­£åœ¨è·å–ç½‘é¡µå†…å®¹...")
    try:
        html = fetch_html()
        print(f"è·å–æˆåŠŸï¼HTML å¤§å°: {len(html):,} å­—èŠ‚")
    except Exception as e:
        print(f"è·å–å¤±è´¥: {e}")
        return

    # æµ‹è¯• html.parser
    html_parser_time, html_parser_avg = benchmark_parser(
        html, "html.parser (Pythonå†…ç½®)", "html.parser", iterations=50
    )

    # æµ‹è¯• lxml
    lxml_time, lxml_avg = benchmark_parser(
        html, "lxml (Cè¯­è¨€å®ç°)", "lxml", iterations=50
    )

    # å¯¹æ¯”ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("="*60)

    speedup = html_parser_avg / lxml_avg
    print(f"\n{'è§£æå™¨':<25} {'å¹³å‡æ—¶é—´':<15} {'ç›¸å¯¹é€Ÿåº¦':<15}")
    print("-" * 60)
    print(f"{'html.parser':<25} {html_parser_avg*1000:>8.2f} ms{'':<7} {'åŸºå‡†':<15}")
    print(f"{'lxml':<25} {lxml_avg*1000:>8.2f} ms{'':<7} {speedup:.1f}x å¿«é€Ÿ")

    print("\n" + "="*60)
    print("ğŸ“ˆ ç»“è®º")
    print("="*60)

    if speedup >= 5:
        print(f"âœ… lxml æ¯” html.parser å¿« {speedup:.1f} å€ï¼")
        print("   å¼ºçƒˆæ¨èä½¿ç”¨ lxml è§£æå™¨")
    elif speedup >= 2:
        print(f"âœ… lxml æ¯” html.parser å¿« {speedup:.1f} å€")
        print("   æ¨èä½¿ç”¨ lxml è§£æå™¨")
    elif speedup >= 1.5:
        print(f"âš ï¸  lxml æ¯” html.parser å¿« {speedup:.1f} å€")
        print("   å¯ä»¥è€ƒè™‘ä½¿ç”¨ lxml")
    else:
        print(f"âŒ ä¸¤è€…æ€§èƒ½ç›¸è¿‘ï¼ˆ{speedup:.1f}xï¼‰")
        print("   å¯ä»¥æ ¹æ®å…¶ä»–å› ç´ é€‰æ‹©")

    print("\n" + "="*60)
    print("ğŸ’¡ å»ºè®®")
    print("="*60)

    if speedup >= 2:
        print("1. âœ… ä½¿ç”¨ lxml - æ€§èƒ½ä¼˜åŠ¿æ˜æ˜¾")
        print("2. âœ… ä½¿ç”¨ lxml - å®¹é”™æ€§æ›´å¥½")
        print("3. âœ… ä½¿ç”¨ lxml - æ”¯æŒ XPath å’Œ CSS é€‰æ‹©å™¨")
        print("4. âš ï¸  å¦‚æœæ— æ³•å®‰è£… lxmlï¼Œä½¿ç”¨ html.parser ä¹Ÿå¯ä»¥")
    else:
        print("1. âœ… ä¸¤è€…çš†å¯ - æ€§èƒ½å·®å¼‚ä¸å¤§")
        print("2. âœ… ä½¿ç”¨ html.parser - æ— éœ€é¢å¤–ä¾èµ–")
        print("3. âœ… ä½¿ç”¨ lxml - åŠŸèƒ½æ›´å¼ºå¤§")


def test_accuracy():
    """æµ‹è¯•ä¸¤ç§è§£æå™¨çš„å‡†ç¡®æ€§"""
    print("\n" + "="*60)
    print("ğŸ” å‡†ç¡®æ€§æµ‹è¯•")
    print("="*60)

    test_words = ["hello", "world", "python", "algorithm"]

    for word in test_words:
        print(f"\næµ‹è¯•å•è¯: {word}")
        print("-" * 40)

        url = f"https://dict.youdao.com/search?q={word}"
        headers = {
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                          "(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        }

        try:
            response = requests.get(url, headers=headers, timeout=10)

            # html.parser
            soup_html = BeautifulSoup(response.text, 'html.parser')
            result_html = soup_html.find('div', id='results-contents')

            # lxml
            soup_lxml = BeautifulSoup(response.text, 'lxml')
            result_lxml = soup_lxml.find('div', id='results-contents')

            # æ¯”è¾ƒç»“æœ
            if result_html and result_lxml:
                html_text = result_html.get_text(strip=True)[:100]
                lxml_text = result_lxml.get_text(strip=True)[:100]

                if html_text == lxml_text:
                    print(f"âœ… ä¸¤è€…ç»“æœä¸€è‡´")
                else:
                    print(f"âš ï¸  ç»“æœç•¥æœ‰å·®å¼‚ï¼ˆæ­£å¸¸ç°è±¡ï¼‰")
                    print(f"   html.parser: {html_text}...")
                    print(f"   lxml: {lxml_text}...")
            elif result_html:
                print(f"âœ… html.parser æ‰¾åˆ°ç»“æœï¼Œlxml æœªæ‰¾åˆ°")
            elif result_lxml:
                print(f"âœ… lxml æ‰¾åˆ°ç»“æœï¼Œhtml.parser æœªæ‰¾åˆ°")
            else:
                print(f"âŒ ä¸¤è€…éƒ½æœªæ‰¾åˆ°ç»“æœ")

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    try:
        compare_parsers()
        test_accuracy()
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•å·²ä¸­æ–­")
    except Exception as e:
        print(f"\n\næµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
